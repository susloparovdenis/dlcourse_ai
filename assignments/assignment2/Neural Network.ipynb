{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/denis/Source/colab/dlcourse_ai/assignments/assignment1\n"
     ]
    }
   ],
   "source": [
    "%cd assignments/assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for FullyConnectedLayer_0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer_0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer_2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer_2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for FullyConnectedLayer_0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer_0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer_2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for FullyConnectedLayer_2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "samples = 900\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:samples]), train_y[:samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_y[:samples]==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.299364, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303996, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306916, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304208, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294550, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291798, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303474, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298647, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302627, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304788, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297501, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300976, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290628, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302745, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291922, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305899, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289555, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300634, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd07cd66430>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.281337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296953, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311361, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.312841, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295474, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241209, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253498, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.341119, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295631, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224670, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262271, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182057, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272200, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.284177, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.320207, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208350, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309415, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.338348, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.311430, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.316921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308512, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.307118, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314216, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.310917, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290527, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259834, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288004, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227918, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288148, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300996, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246914, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.322529, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288680, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261971, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285712, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245889, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285073, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331619, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.336105, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.321334, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.331523, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.294044, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.257004, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.298353, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.349459, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.279768, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.960522, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.928829, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.982798, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.218993, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.015133, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.962675, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.426123, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.822621, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.946312, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.731569, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.122162, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.757687, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.756944, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.550047, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.471494, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.849322, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.094404, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.889466, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.053481, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.168377, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.386667, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.936363, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.611942, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.980843, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.687137, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.766181, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.226302, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.452529, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.620547, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.664671, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.773169, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.974947, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.941742, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.199350, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.365904, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.050435, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.826106, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.995411, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.232229, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.598591, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.683696, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.488535, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.837487, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.615028, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.956633, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.095733, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.468586, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.636529, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.865111, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.302042, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.682677, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.110165, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.747626, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.674309, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.308649, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.339179, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.168453, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.294782, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.016562, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.638504, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.981930, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.951172, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.326753, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.613736, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.656068, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.895519, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.694498, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.656207, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.231373, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.850952, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.030511, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.651746, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.308095, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.657759, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.284165, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.667570, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.393020, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.932146, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.315353, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.637003, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.342179, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.393454, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.340114, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.654904, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.124610, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.082715, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.283445, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.918388, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.220978, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.182456, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.786293, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.826134, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.390726, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.004250, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.332182, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.088247, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.199570, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.276655, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.677136, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.477137, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.203887, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.453137, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.461487, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.261173, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.019665, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.319885, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.153449, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.449272, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.368123, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.082283, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.972001, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.547196, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.117870, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.355211, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.466750, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.302657, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.289473, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.136708, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.219108, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.053260, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.237531, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.056663, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.390735, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.225059, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.376369, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.185635, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.132326, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.355696, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.134266, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.253288, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.333920, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.200495, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.324620, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.245064, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.141008, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.276993, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.216704, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.199682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.375885, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.190198, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.278186, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327764, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.315370, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.235740, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.894580, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.349224, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.254980, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.193153, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.431534, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 1.953475, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.400409, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.280600, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.040854, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.642206, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.953880, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.244612, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.336350, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.484805, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.612119, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.271741, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.276242, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 400, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.173501, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.100491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278455, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.080153, Train accuracy: 0.261000, val accuracy: 0.259000\n",
      "Loss: 2.029098, Train accuracy: 0.311000, val accuracy: 0.315000\n",
      "Loss: 1.510735, Train accuracy: 0.391000, val accuracy: 0.388000\n",
      "Loss: 1.827691, Train accuracy: 0.456667, val accuracy: 0.455000\n",
      "Loss: 1.613189, Train accuracy: 0.519333, val accuracy: 0.517000\n",
      "Loss: 1.156851, Train accuracy: 0.570444, val accuracy: 0.562000\n",
      "Loss: 1.442326, Train accuracy: 0.605889, val accuracy: 0.607000\n",
      "Loss: 1.447743, Train accuracy: 0.633222, val accuracy: 0.630000\n",
      "Loss: 1.191157, Train accuracy: 0.653556, val accuracy: 0.643000\n",
      "Loss: 1.219033, Train accuracy: 0.686000, val accuracy: 0.675000\n",
      "Loss: 0.856997, Train accuracy: 0.701111, val accuracy: 0.667000\n",
      "Loss: 0.980743, Train accuracy: 0.713000, val accuracy: 0.693000\n",
      "Loss: 1.060762, Train accuracy: 0.711667, val accuracy: 0.690000\n",
      "Loss: 0.891114, Train accuracy: 0.723889, val accuracy: 0.701000\n",
      "Loss: 1.046663, Train accuracy: 0.740111, val accuracy: 0.704000\n",
      "Loss: 1.061423, Train accuracy: 0.743889, val accuracy: 0.699000\n",
      "Loss: 1.014835, Train accuracy: 0.749333, val accuracy: 0.707000\n",
      "Loss: 0.994358, Train accuracy: 0.759667, val accuracy: 0.711000\n",
      "Loss: 0.936966, Train accuracy: 0.766111, val accuracy: 0.715000\n",
      "Loss: 0.671983, Train accuracy: 0.772222, val accuracy: 0.718000\n",
      "Loss: 0.980975, Train accuracy: 0.776778, val accuracy: 0.722000\n",
      "Loss: 1.021521, Train accuracy: 0.771111, val accuracy: 0.709000\n",
      "Loss: 0.951242, Train accuracy: 0.790222, val accuracy: 0.729000\n",
      "Loss: 0.845335, Train accuracy: 0.777000, val accuracy: 0.724000\n",
      "Loss: 1.079344, Train accuracy: 0.792444, val accuracy: 0.724000\n",
      "Loss: 0.889453, Train accuracy: 0.797667, val accuracy: 0.723000\n",
      "Loss: 1.070358, Train accuracy: 0.796556, val accuracy: 0.724000\n",
      "Loss: 0.854245, Train accuracy: 0.797222, val accuracy: 0.718000\n",
      "Loss: 1.207376, Train accuracy: 0.781222, val accuracy: 0.730000\n",
      "Loss: 0.971825, Train accuracy: 0.819111, val accuracy: 0.738000\n",
      "Loss: 0.945378, Train accuracy: 0.814889, val accuracy: 0.743000\n",
      "Loss: 0.885243, Train accuracy: 0.819556, val accuracy: 0.743000\n",
      "Loss: 0.687755, Train accuracy: 0.822667, val accuracy: 0.739000\n",
      "Loss: 1.155878, Train accuracy: 0.822889, val accuracy: 0.737000\n",
      "Loss: 0.910004, Train accuracy: 0.826333, val accuracy: 0.727000\n",
      "Loss: 0.899611, Train accuracy: 0.835222, val accuracy: 0.739000\n",
      "Loss: 1.096281, Train accuracy: 0.829556, val accuracy: 0.748000\n",
      "new leader with val_acc=0.748 and params: [hidden_layer_size=100 reg_strength=0.001 learning_rate=0.01]\n",
      "Loss: 2.155209, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230409, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.075884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231944, Train accuracy: 0.260222, val accuracy: 0.258000\n",
      "Loss: 1.962325, Train accuracy: 0.268778, val accuracy: 0.274000\n",
      "Loss: 1.768841, Train accuracy: 0.361444, val accuracy: 0.385000\n",
      "Loss: 1.544708, Train accuracy: 0.453111, val accuracy: 0.458000\n",
      "Loss: 1.429341, Train accuracy: 0.526444, val accuracy: 0.520000\n",
      "Loss: 1.426148, Train accuracy: 0.581444, val accuracy: 0.571000\n",
      "Loss: 1.093414, Train accuracy: 0.625111, val accuracy: 0.605000\n",
      "Loss: 1.361298, Train accuracy: 0.648889, val accuracy: 0.632000\n",
      "Loss: 1.462385, Train accuracy: 0.671000, val accuracy: 0.643000\n",
      "Loss: 1.126864, Train accuracy: 0.703889, val accuracy: 0.688000\n",
      "Loss: 0.936911, Train accuracy: 0.712889, val accuracy: 0.690000\n",
      "Loss: 0.851592, Train accuracy: 0.725889, val accuracy: 0.698000\n",
      "Loss: 0.795364, Train accuracy: 0.729667, val accuracy: 0.688000\n",
      "Loss: 0.865690, Train accuracy: 0.738333, val accuracy: 0.704000\n",
      "Loss: 0.888329, Train accuracy: 0.747111, val accuracy: 0.702000\n",
      "Loss: 0.821756, Train accuracy: 0.754778, val accuracy: 0.709000\n",
      "Loss: 0.972160, Train accuracy: 0.756000, val accuracy: 0.719000\n",
      "Loss: 1.017408, Train accuracy: 0.761778, val accuracy: 0.701000\n",
      "Loss: 1.060084, Train accuracy: 0.769889, val accuracy: 0.697000\n",
      "Loss: 0.786075, Train accuracy: 0.783889, val accuracy: 0.725000\n",
      "Loss: 0.538270, Train accuracy: 0.789000, val accuracy: 0.723000\n",
      "Loss: 0.759201, Train accuracy: 0.797000, val accuracy: 0.717000\n",
      "Loss: 0.608310, Train accuracy: 0.807000, val accuracy: 0.729000\n",
      "Loss: 0.964500, Train accuracy: 0.800333, val accuracy: 0.720000\n",
      "Loss: 0.301789, Train accuracy: 0.802889, val accuracy: 0.712000\n",
      "Loss: 0.689016, Train accuracy: 0.816111, val accuracy: 0.738000\n",
      "Loss: 0.669288, Train accuracy: 0.815556, val accuracy: 0.720000\n",
      "Loss: 0.597664, Train accuracy: 0.832222, val accuracy: 0.739000\n",
      "Loss: 0.652848, Train accuracy: 0.837778, val accuracy: 0.739000\n",
      "Loss: 0.549471, Train accuracy: 0.828778, val accuracy: 0.733000\n",
      "Loss: 0.600733, Train accuracy: 0.850222, val accuracy: 0.749000\n",
      "Loss: 0.910095, Train accuracy: 0.847444, val accuracy: 0.732000\n",
      "Loss: 0.616163, Train accuracy: 0.832333, val accuracy: 0.735000\n",
      "Loss: 0.742702, Train accuracy: 0.853667, val accuracy: 0.740000\n",
      "Loss: 0.770250, Train accuracy: 0.865333, val accuracy: 0.746000\n",
      "Loss: 0.520315, Train accuracy: 0.861333, val accuracy: 0.752000\n",
      "Loss: 0.715675, Train accuracy: 0.836000, val accuracy: 0.729000\n",
      "Loss: 2.189987, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206782, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.144455, Train accuracy: 0.258556, val accuracy: 0.255000\n",
      "Loss: 2.050166, Train accuracy: 0.313778, val accuracy: 0.322000\n",
      "Loss: 1.578650, Train accuracy: 0.396889, val accuracy: 0.395000\n",
      "Loss: 1.644339, Train accuracy: 0.468444, val accuracy: 0.487000\n",
      "Loss: 1.386531, Train accuracy: 0.529333, val accuracy: 0.525000\n",
      "Loss: 1.538660, Train accuracy: 0.572444, val accuracy: 0.571000\n",
      "Loss: 1.345682, Train accuracy: 0.590000, val accuracy: 0.595000\n",
      "Loss: 1.023484, Train accuracy: 0.635000, val accuracy: 0.631000\n",
      "Loss: 0.980650, Train accuracy: 0.658889, val accuracy: 0.650000\n",
      "Loss: 0.927488, Train accuracy: 0.665444, val accuracy: 0.658000\n",
      "Loss: 1.366763, Train accuracy: 0.695222, val accuracy: 0.688000\n",
      "Loss: 1.071604, Train accuracy: 0.713000, val accuracy: 0.697000\n",
      "Loss: 1.024564, Train accuracy: 0.710000, val accuracy: 0.697000\n",
      "Loss: 1.114700, Train accuracy: 0.720889, val accuracy: 0.697000\n",
      "Loss: 1.064088, Train accuracy: 0.727333, val accuracy: 0.707000\n",
      "Loss: 1.210748, Train accuracy: 0.743333, val accuracy: 0.711000\n",
      "Loss: 0.838345, Train accuracy: 0.749111, val accuracy: 0.705000\n",
      "Loss: 1.105753, Train accuracy: 0.750000, val accuracy: 0.699000\n",
      "Loss: 1.052289, Train accuracy: 0.753000, val accuracy: 0.711000\n",
      "Loss: 1.001504, Train accuracy: 0.778667, val accuracy: 0.716000\n",
      "Loss: 0.905426, Train accuracy: 0.779889, val accuracy: 0.722000\n",
      "Loss: 0.918795, Train accuracy: 0.769222, val accuracy: 0.715000\n",
      "Loss: 1.050023, Train accuracy: 0.780333, val accuracy: 0.718000\n",
      "Loss: 1.270288, Train accuracy: 0.799889, val accuracy: 0.728000\n",
      "Loss: 0.892044, Train accuracy: 0.795778, val accuracy: 0.727000\n",
      "Loss: 0.869111, Train accuracy: 0.798000, val accuracy: 0.739000\n",
      "Loss: 0.957574, Train accuracy: 0.805889, val accuracy: 0.740000\n",
      "Loss: 0.848559, Train accuracy: 0.808444, val accuracy: 0.737000\n",
      "Loss: 0.708257, Train accuracy: 0.809778, val accuracy: 0.738000\n",
      "Loss: 0.757501, Train accuracy: 0.814778, val accuracy: 0.738000\n",
      "Loss: 1.124183, Train accuracy: 0.823444, val accuracy: 0.745000\n",
      "Loss: 0.737677, Train accuracy: 0.815444, val accuracy: 0.740000\n",
      "Loss: 0.966418, Train accuracy: 0.834889, val accuracy: 0.763000\n",
      "Loss: 0.945079, Train accuracy: 0.833222, val accuracy: 0.738000\n",
      "Loss: 0.839611, Train accuracy: 0.839889, val accuracy: 0.742000\n",
      "Loss: 0.798637, Train accuracy: 0.844333, val accuracy: 0.755000\n",
      "Loss: 0.873607, Train accuracy: 0.849444, val accuracy: 0.754000\n",
      "new leader with val_acc=0.754 and params: [hidden_layer_size=120 reg_strength=0.001 learning_rate=0.01]\n",
      "Loss: 2.157766, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.149049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.087037, Train accuracy: 0.203000, val accuracy: 0.217000\n",
      "Loss: 1.992909, Train accuracy: 0.271333, val accuracy: 0.268000\n",
      "Loss: 1.641844, Train accuracy: 0.320667, val accuracy: 0.328000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_405116/2936405113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mval_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"new leader with val_acc={val_history[-1]} and params: [{hidden_layer_size=} {reg_strength=} {learning_rate=}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/colab/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/colab/dlcourse_ai/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Hint: self.params() is useful again!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mloss_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_l2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad_l2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/colab/dlcourse_ai/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36ml2_regularization\u001b[0;34m(W, reg_strength)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0msame\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mof\u001b[0m \u001b[0mweight\u001b[0m \u001b[0mby\u001b[0m \u001b[0ml2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg_strength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg_strength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-2]\n",
    "reg_strengths = [1e-3, 1e-4]\n",
    "learning_rate_decay = 0.995\n",
    "hidden_layer_sizes = [100, 120, 150]\n",
    "num_epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    for reg_strength in reg_strengths:\n",
    "        for learning_rate in learning_rates:\n",
    "            model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "            dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "            trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=learning_rate, num_epochs=num_epochs, batch_size=batch_size, learning_rate_decay=learning_rate_decay)\n",
    "\n",
    "            loss_history, train_history, val_history = trainer.fit()\n",
    "            if best_val_accuracy < val_history[-1]:\n",
    "                print(f\"new leader with val_acc={val_history[-1]} and params: [{hidden_layer_size=} {reg_strength=} {learning_rate=}]\")\n",
    "                best_val_accuracy = val_history[-1]\n",
    "                best_classifier = model\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd07c892e50>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.723000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "new leader with val_acc=0.759 and params: [hidden_layer_size=150 reg_strength=0.001 learning_rate=0.01]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

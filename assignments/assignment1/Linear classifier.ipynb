{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:\n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy\n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "\n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "\n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])\n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.\n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "\n",
    "    x: np array of float, input to the function\n",
    "\n",
    "    Returns:\n",
    "    value: float, value of the function\n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return x*x, 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.\n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0.\n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([1]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов.\n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n",
      "Final epoch 10, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3973628528706006,\n",
       " 2.33035363385622,\n",
       " 2.311001879530181,\n",
       " 2.303896768558966,\n",
       " 2.3032573562390555,\n",
       " 2.3028982205245367,\n",
       " 2.3025637241358172,\n",
       " 2.301814898640508,\n",
       " 2.3012523952713,\n",
       " 2.30125583335474]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc409b4b610>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Final epoch 100, loss: 2.302352\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final epoch 100, loss: 2.158999\n",
      "current best accuracy: 0.242 with params: {'reg': 0.001, 'lr': 0.01}\n",
      "Final epoch 100, loss: 2.137847\n",
      "Final epoch 100, loss: 2.097818\n",
      "Final epoch 100, loss: 2.147794\n",
      "Final epoch 100, loss: 2.155736\n",
      "Final epoch 100, loss: 2.243153\n",
      "Final epoch 100, loss: 2.235280\n",
      "Final epoch 100, loss: 2.254261\n",
      "Final epoch 100, loss: 2.240974\n",
      "Final epoch 100, loss: 2.250720\n",
      "Final epoch 100, loss: 2.296159\n",
      "Final epoch 100, loss: 2.292005\n",
      "Final epoch 100, loss: 2.296454\n",
      "Final epoch 100, loss: 2.294843\n",
      "Final epoch 100, loss: 2.293651\n",
      "Final epoch 100, loss: 2.301360\n",
      "Final epoch 100, loss: 2.301777\n",
      "Final epoch 100, loss: 2.302179\n",
      "Final epoch 100, loss: 2.301726\n",
      "Final epoch 100, loss: 2.302726\n",
      "Final epoch 100, loss: 2.302537\n",
      "Final epoch 100, loss: 2.301871\n",
      "Final epoch 100, loss: 2.303479\n",
      "Final epoch 100, loss: 2.303072\n",
      "Final epoch 100, loss: 2.302349\n",
      "best validation accuracy achieved: 0.242 with params: {'reg': 0.001, 'lr': 0.01}\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength\n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs, batch_size=batch_size, learning_rate=lr, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy < accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_params = {'reg': reg, 'lr': lr}\n",
    "            print(f'current best accuracy: {best_val_accuracy} with params: {best_params}')\n",
    "\n",
    "print(f'best validation accuracy achieved: {best_val_accuracy} with params: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.\n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.242"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.201000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
